{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucmbIwckePeW",
        "outputId": "34723407-0772-4e30-9c9a-9f718a283e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why it's useful:\n",
        "\n",
        "You don't have to manually download a model, preprocess inputs, or write complex model interaction code. The pipeline handles all that for you.\n",
        "\n",
        "What it does:\n",
        "\n",
        "The pipeline function is called to create a pipeline object.\n",
        "The \"text-generation\" argument specifies the type of task the pipeline will perform. In this case, it's text generation, which means the model will generate text based on input prompts.\n",
        "\n",
        "The model=\"openai-community/gpt2\" argument specifies the model to be used. The openai-community/gpt2 is a publicly available GPT-2 model hosted on Hugging Face."
      ],
      "metadata": {
        "id": "fHX3y1-sWu9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**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**"
      ],
      "metadata": {
        "id": "rr7eu4zrqynH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"Hi, I am not doing great today\")[0][\"generated_text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "B1fBDHpAiHXQ",
        "outputId": "63ee1f74-1239-4cc5-a59f-a244c3a366b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi, I am not doing great today. I am too lazy.\"\\n\\nBut with all this talk about how he doesn\\'t make music, or how he\\'s the one who decides to turn down shows and then come back, not only do many'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-Step Explanation:\n",
        "Pipeline Call:\n",
        "\n",
        "You pass the input text \"Hi, I am not doing great today\" to the pipe object, which is a text-generation pipeline.\n",
        "\n",
        "This input text acts as the prompt for the model to generate additional text.\n",
        "Processing by the Pipeline:\n",
        "\n",
        "The pipeline does the following behind the scenes:\n",
        "Tokenization: It converts the input string into tokens (numerical representations).\n",
        "\n",
        "Model Inference: The model processes these tokens and predicts the most likely next tokens based on its training data.\n",
        "\n",
        "Decoding: The predicted tokens are converted back into a human-readable string.\n",
        "Generated Output:\n",
        "\n",
        "The pipeline returns a list of dictionaries, where each dictionary contains the generated text and any metadata (like confidence scores).\n",
        "\n",
        "You are accessing the first item in the list ([0]) and specifically retrieving the \"generated_text\" key from that dictionary.\n"
      ],
      "metadata": {
        "id": "OskiL315XDr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/openai-community/gpt2"
      ],
      "metadata": {
        "id": "ZznefOnShTIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "qouhYqR5hA_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoTokenizer:\n",
        "\n",
        "A class for loading the tokenizer associated with the model.\n",
        "The tokenizer is responsible for splitting text into tokens (input to the model) and decoding tokens back into text (output from the model).\n",
        "\n",
        "\n",
        "AutoModelForCausalLM:\n",
        "\n",
        "A class for loading causal language models (like GPT-2).\n",
        "Causal Language Models predict the next token in a sequence, making them suitable for tasks like text generation."
      ],
      "metadata": {
        "id": "FlSp2JirXQwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What It Does:\n",
        "\n",
        "Downloads the tokenizer for the openai-community/gpt2 model from the Hugging Face Hub (if not already cached).\n",
        "Loads the tokenizer into memory so it can process input text.\n",
        "\n",
        "What It Does:\n",
        "\n",
        "Downloads the openai-community/gpt2 model weights from the Hugging Face Hub (if not already cached).\n",
        "Loads the model into memory for inference.\n",
        "Behind the Scenes:\n",
        "\n",
        "The model architecture (e.g., number of layers, hidden units) and pre-trained weights are fetched.\n",
        "The model is instantiated as a PyTorch or TensorFlow object (depending on your environment).\n",
        "Result:\n",
        "\n",
        "The model object is ready to process tokenized input and generate tokenized output.\n"
      ],
      "metadata": {
        "id": "w9Aj6RHAXgCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT8KuhJUmbaH",
        "outputId": "f3f52717-f57b-4b99-d1e2-0a49fc2705f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              " \t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              " }\n",
              " ),\n",
              " GPT2LMHeadModel(\n",
              "   (transformer): GPT2Model(\n",
              "     (wte): Embedding(50257, 768)\n",
              "     (wpe): Embedding(1024, 768)\n",
              "     (drop): Dropout(p=0.1, inplace=False)\n",
              "     (h): ModuleList(\n",
              "       (0-11): 12 x GPT2Block(\n",
              "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "         (attn): GPT2SdpaAttention(\n",
              "           (c_attn): Conv1D(nf=2304, nx=768)\n",
              "           (c_proj): Conv1D(nf=768, nx=768)\n",
              "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "         (mlp): GPT2MLP(\n",
              "           (c_fc): Conv1D(nf=3072, nx=768)\n",
              "           (c_proj): Conv1D(nf=768, nx=3072)\n",
              "           (act): NewGELUActivation()\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "       )\n",
              "     )\n",
              "     (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "   )\n",
              "   (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The brown dog is sleeping on black mat'"
      ],
      "metadata": {
        "id": "ufsIx05LnARz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs= tokenizer(text, return_tensors=\"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xsSL0h8ns1A",
        "outputId": "6b4664f1-ec05-4b94-9f52-36446b3e7f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  464,  7586,  3290,   318, 11029,   319,  2042,  2603]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(**inputs)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9L8bnDXoKl_",
        "outputId": "e9b8a77d-b3e1-45ce-f8ec-f424c2c9d913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  464,  7586,  3290,   318, 11029,   319,  2042,  2603,    13,   198,\n",
              "           198,   464,  7586,  3290,   318, 11029,   319,  2042,  2603,    13,\n",
              "           383,  7586,  3290,   318, 11029,   319,  2042,  2603]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=tokenizer.decode(output[0])\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "E-oHeJ7joo86",
        "outputId": "ba81bb4b-21c5-42e6-bece-5e3e73e68364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown dog is sleeping on black mat.\\n\\nThe brown dog is sleeping on black mat. The brown dog is sleeping on black mat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = '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'"
      ],
      "metadata": {
        "id": "cygqrr8YyvGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/openai-community/gpt2\"\n",
        "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": \"Can you please let us know more details about yourself \",\n",
        "})"
      ],
      "metadata": {
        "id": "JFYRtHlghDgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdFbMniDprHa",
        "outputId": "bfa4bc08-ab77-4598-9e49-0003bbf20f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Can you please let us know more details about yourself ?\"\\n\\n\"Yeah, I know too. I just … went of my mind, probably overheated.\"\\n\\nJudy was honestly surprised to hear Sgt. Abdul Saqeda\\n\\n\"Well, whatever that…….\\n\\nHuh. What\\'s that next thing really mean? You\\'ve got to leave their guy in the wrong area.\"\\n\\n\"Pfft wil never go. He knows that\\'s it I\\'ll be leaving behind.\"\\n\\nEven Brandy and Naido were starting to complain about Hoshiko as an infuriated Naido called out to Lillian.\\n\\nGuess they all needed him to associate with them. They just needed them to let her listen to stories mixed with wisdom and like comparisons. What didn\\'t Lillian listen to Gerard as a small child?\\n\\n\"Who\\'d ever do that? And what the hell …?\"\\n\\nShe knew there really wasn\\'t anything wrong with all the time spent with them\\'s all grown\\'s sappers. Who childhood strip pimps need to do.\\n\\nLisa\\'s sister allowed it, but she did know something important. Apparently the Institute extended remote control capabilities to other vaca-dono have home audience\\'s.\\n\\n\"What\\'s with that window, does it have the mirror?\"\\n\\n\"It doesn\\'t yet. No carpenter robots smash through those windows at full postures impact radar\\n\\nNifeng\\'s eyes widened as she traded in her goggles for studded skiffs that gig\\'d be babysitting.\\n\\nThat was right, and in the Etsy shop below they all bought giddy together and embraced each other with bountiful hearts. Good time Gaia.\\n\\nAt this time, Mick flew to Eli\\'s place but returned prematurely. What kind of broken man was running to some random casket so later couldn\\'t pick up the kiss?\\n\\n2 nd Hoshiko was happy to see its daughter.\\n\\nHe was pitiable at first, but after shaking his head slowly, he saw that she was dropping up on Haida and had decided to take his place.\\n\\nShe was so cute too too.\\n\\nThen again, because she was M3 Allison Dinsene with green eyes, it might have been weird even for Mick.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}