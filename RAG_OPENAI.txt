Generative AI & Retrieval-Augmented Generation (RAG) Models

This repository contains a collection of Large Language Model (LLM) applications, including text generation, fine-tuning, summarization, and Retrieval-Augmented Generation (RAG) with OpenAI. The notebooks provide end-to-end implementations using Hugging Face Transformers, LangChain, and Pinecone.


1️. Transformers_GPT2- Text Generation with GPT-2:

Uses GPT-2 from Hugging Face for text generation.
Implements the pipeline function for simplified model interaction.
Libraries Used: transformers, requests, google.

2️. Text_Completion_Fine_Tuning_LLMs - Fine-Tuning LLMs for Text Completion:

Explains pre-training vs fine-tuning concepts.
Uses Hugging Face’s Trainer API for model adaptation.
Libraries Used: transformers, datasets, os.

3️. Text_Summarization_Model_LLMS - Summarization using Encoder-Decoder Models:

Covers encoder-decoder architectures for structured input-output tasks.
Uses Hugging Face Transformers for text summarization.
Libraries Used: transformers.

4️. RAG_OPENAI - Retrieval-Augmented Generation (RAG) with OpenAI:

Implements LangChain and Pinecone for knowledge retrieval.
Demonstrates vector database indexing and retrieval from a large knowledge base.
Libraries Used: langchain, pinecone, google, os, time

Applications & Use Cases:

Text Generation: GPT-2 generates human-like text.
Fine-Tuning: Customizing LLMs for domain-specific tasks.
Summarization: Condensing long documents into concise summaries.
RAG (Retrieval-Augmented Generation): Enhancing LLMs with external knowledge retrieval.